---
title: "Week 1 Lab:  Becoming familiar with R, RStudio, RMarkdown"
author: "ENVH 556 Autumn 2024 Instructors"
format: 
    html:
        df_print: "paged"
        fig_caption: yes
        toc: true
        toc_depth: 3
        number_sections: true
        #self-contained: true #save images etc. in this file (vs folders)
execute:
  echo: true
  cache: false
  echo.comments: false
  message: false
  warning: false
  tidy: false
---

This document was rendered on `r format(Sys.time(), '%B %d, %Y')`.

------------------------------------------------------------------------

Note: The code in the following three chunks should typically appear at the beginning of each Markdown file. We display them during this first lab so you can see them in the output. Ordinarily we would not show them.

```{r setup, include=TRUE}
# this "hash pipe" format also works with quatro
#| include: true

#-----setup-------------

# We are showing the code in this chunk in the first lab; ordinarily
# we will choose to hide it with the option echo=FALSE. 
#
# Note about naming chunks:
#
# The chunk name is useful for reading code, for the index you can choose to
# show in RStudio to the right of the script editor, and for more advanced
# purposes.  The chunk name will not show up in your Appendix code compilation
# however.  The comment with the chunk name between dashes will both show up in
# the index (boldfaced) and print in your appendix.  This facilitates review of
# the code. I recommend using chunk names and putting them at least as a
# comment at the beginning of the chunk.

# the yaml header includes global options

```

```{r clear.workspace, eval=FALSE, echo=TRUE}
#-----clear workspace------------

# Clear the environment without clearing knitr
#
# This chunk is useful for code development because it simulates the knitr
# environment. Run it as a code chunk when testing. When knitr is run, it uses a
# fresh, clean environment, so we set eval=FALSE to disable this chunk when
# rendering.

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
    res <- suppressWarnings(
        lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
               detach, character.only=TRUE, unload=TRUE, force=TRUE))
   
}

```

```{r load.packages.with.pacman, include=TRUE}
#-----load packages with pacman--------------

# Ordinarily we set the chunk header to include=FALSE for this chunk; just 
# showing it for the first lab.

# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.  As you progress, it is good
# practice to load specific packages, but for now we'll load all of the 
# `tidyverse` for convenience. Some reasons for packages:
# knitr:  kable()
# ggplot2: Don't load:  part of tidyverse
# readr:   Don't load:  part of tidyverse
# dplyr:   Don't load:  part of tidyverse
# Hmisc:  Miscellaneous functions helpful for data analysis
# EnvStats: geoMean, geoSD, probability plotting functions
pacman::p_load(tidyverse, knitr, Hmisc, EnvStats)

source("global_functions.R")

```

------------------------------------------------------------------------

# Goal

The goal of this lab is to help everyone become familiar with R, R Studio, and Markdown through the context of the course content. This is also your first opportunity to develop a lab write-up for ENVH 556 using Markdown.

Note: It is in your best interest to learn modern R tools, such as those included in the [`tidyverse`](https://www.tidyverse.org). In this class we tend to prioritize `tidyvrese` approaches, but recognize there are often many packages and options to accomplish the same task. We want to deliver course materials without getting too bogged down in the debate about the "best" or "highest performing" packages.

------------------------------------------------------------------------

# Practice Session

This section covers basic practice to be completed during the lab. It will introduce you to multiple useful commands and help you get a basic understanding of the dataset.

The underlying data to be used in the lab is from the Seattle Mobile Monitoring study and is described in [Blanco et al. 2022](https://pubs.acs.org/doi/full/10.1021/acs.est.2c01077), as discussed in class. We are focusing on the annual average site concentration data here.  

A big part of data science is data management and ideally you will also learn basic data management in conjunction with your work. This class will not emphasize data management, although you will do some during your term project. Note that some of the habits we stress, including those we discuss in this lab, are very useful for both data management and data analyses.

## Set up a RStudio project, file paths, and read the data

Complete the following steps to set up the lab

-   Start a new R project in R Studio.
    -   Note: Put your project for this course (and lab) in a sensible directory.
    -   Copy this lab's `.qmd` file, *Week1Lab.qmd* into your project directory and open it into RStudio.
    -   Note: Often you will open a new `.qmd` (Markdown) file and populate it with code and text for this course, such as from the *LabReportGuidelines.qmd* file.
-   In the Markdown file you are using for this lab make sure you have your file paths set up (see *set file paths* chunk below).
    -   First assign your current path to a variable name.
    -   Then tell R where your data reside. The `dir.create()` command will make a directory if it doesn't exist. The `file.path()` command allows you to refer easily to paths across operating systems.

```{r set.file.paths}
#-----set file paths--------

# where data is/will be stored
data_path <- c(file.path("Datasets", "mobile monitoring"))

# create directories if they do not already exist    
if(!dir.exists(data_path)){dir.create(data_path, showWarnings=FALSE, recursive = TRUE)}


```

-   Read in the data

```{r read.data, eval=FALSE, error=TRUE}
#-----read data--------

# Note 1:  for simplicity of typing in this lab we're calling the dataset DEMS,
# even though it is only one of several DEMS datasets we will use in this
# course. 
# Note 2: In this chunk heading eval = FALSE so that if it is run, we don't get an 
# error if the file is not present in the "Datasets" folder. This would work if 
# the `.rds` file were already downloaded and is a bit simpler compared to the 
# more robust but more complicated code below.  
DEMS <- readRDS(file.path(data_path, "DEMSCombinedPersonal.rds"))

```

As an alternative, the following code checks to see if the file is already present, and if not, downloads the data from a web site before reading it in:

```{r read.data.from.a.website}
#-----read data from a website--------

# read in annual average air pollution concentrations from mobile monitoring
# download it from the web and save locallyif the file does not already exist
annual <- download_and_read_file(data_url = "https://zenodo.org/record/13761282/files/annual_data_and_predictions.csv?download=1",
                                 # where new file will be saved if it does not already exist or read in from if it does
                                 output_file_path = file.path(data_path, "annual_data_and_predictions.csv"))

```

## Check your data first!

An essential part of any data analysis is making sure the data you are analyzing are as you expect. Before you start any new data analysis, make sure your data have been read in correctly and get a basic understanding of their structure.

### Check to make sure your data have been read in correctly

Here are some basic questions you should ask every time you read in a new dataset:

-   How many observations are in your dataset? Does this number correspond to the originating dataset?
-   What are the variable names in the file? How are they formatted?

#### Here are some commands that will allow you to answer these questions:

From base R:

-   `class(my_dataset)` tells you what class the dataset belongs to
-   `names(my_dataset)` lists the variable names
-   `dim(my_dataset)` gives the dimensions of the dataset
-   `sapply(my_dataset, class)` tells you the class of every variable (column)
-   `typeof(my_dataset)` says what is the storage mode or R internal type of this object
-   `View(my_dataset)` to open a browser to look at the entire dataset

Replace the comment in the chunk below with some of these commands.

```{r data.description.1}
#-----data description 1---------- 

# STUDENTS may want to edit this chunk to look at the data differently

# Here is a first pass at commands to use to check your data we expect this to be a data frame
class(annual) 

# rows and columns in the dataset
dim(annual) 

# column names
names(annual) 

# This gives us details on the type of each variable in the dataset
sapply(annual, class) 

# Now for a tidyverse option.  We will also save this version in case we want to
# use tidverse commands.

# Now lets make the data frame into a tibble Tibbles are tidyverse 
# dataframes with some extra functionality and a friendly display. Occasionally 
# you will run into functions that will work with "data.frame" objects but not 
# "tibble" objects. 
# (The parentheses surrounding the command tell R to print the result of the 
#assignment.)
(annual <- as_tibble(annual)) 

# select a subset of relevant columns
annual <- annual %>%
  select(location, variable, value) 

```

### Get some basic understanding of the data

Here are some commands:

-   `head(my_dataset)` shows the first 6 rows of the dataset
-   `head(my_dataset$facilityno, 20)` shows the first 20 rows of a subset of variables
-   `tail(my_dataset, 1)` shows the last row of the dataset
-   `summary(my_dataset)` gives a basic summary of each variable
-   `describe(my_dataset)` is another useful basic summary of the dataset, from the Hmisc package ("a concise statistical description")
-   `xtabs(~variable_of_interest, data=my_dataset)` tally the number of observations a variable

```{r data.description.2}
#-----data description 2-------

# Here is a first pass at commands for students to try to better understand the data
# # first 6 rows and ALL variables (columns):
head(annual) 

# a basic summary of each variable
summary(annual) 

# a different basic summary of each variable from `Hmisc`
describe(annual) 

# tallies of the number of observations for 'variable' (pollutant)
xtabs( ~ variable, data = annual) 

```

### Verify your data correspond to what you expect

The specifics of what you evaluate depend upon the context of the problem. In our case we have published papers we can rely upon to determine whether our data are what we expect. Here are some questions for the DEMS personal data:

-   How many observations are in your dataset overall and by facility? Do these numbers correspond to the originating datasets? (You can check against the papers.)
-   Note that there are missing data in this dataset. As you proceed, you will want to verify whether they are consistent with your expectations. Presence of missing data requires additional attention to how you will handle these values in R.
    -   Which variables have missing values?
    -   Which variable(s) have the most missing data?

```{r data.description.3}
#-----data description 3------------------

# STUDENTS ADD code to look at the data:
# Which commands from above will allow you to verify observation numbers and
# missing value counts?
# In this dataset it is important to understand summaries by pollutant.  Which
# commands from above will facilitate that?  (We will learn more below too.)

```

## Basic data description

Once you have a basic understanding of your data and believe they were read in correctly, you can focus on quantities of interest. We will focus on creating a few basic summaries of `ecdata` in this lab. Next week you will get more experience with other tools such as transformations and checking the distirbution of variables.

### Tables and computing descriptive statistics

-   What do you observe about the distribution?
-   Do the descriptive statistics vary by facility?

Here are some commands:

-   `min(my_dataset$variable_name,na.rm=TRUE)` is the minimum after missing values omitted
-   `mean(my_dataset$variable_name,na.rm=TRUE)` is the mean after missing values omitted
-   `sd(my_dataset$variable_name,na.rm=TRUE)` is the standard deviation after missing values omitted
-   `summaryFull(my_dataset[,c("variable1", "variable2)])` is a full set of summary statistics from the EnvStats package. It only works for numeric variables.
-   `fivenum(my_dataset$variable_name)` shows the five number summary of a dataset (min, lower hinge (close to the 25th percentile), median, upper hinge, maximum). Note no need to tell it to remove missing missing values here. Five number summaries are used in box plots and give you a basic understanding of the distribution of a variable.
-   From the EnvStats package, `geoMean(my_dataset,na.rm=TRUE)` gives the geometric mean while `geoSD(my_dataset,na.rm=TRUE)` gives the GSD.

To produce summary statistics, here is a `tidyverse` option using `dplyr`:

```{r table.with.dplyr}
#-----table with dplyr------

# dplyr uses verbs for function names. You'll get more comfortable with them as you practice. Also, watch spelling, for instance, `summarise()` versus `summarize()`. In this case summarize()` may call the `Hmisc` function rather than the `dplyr` function. Note: there are many ways to summarize data. This is one approach. 

annual_summary <- annual %>% 
  group_by(variable) %>% 
  summarise(N = sum(!is.na(value)),
            Nmiss = sum(is.na(value)),
            mean = mean(value, na.rm = TRUE),
            sd = sd(value, na.rm = TRUE),
            se = sd/sqrt(N)
            )

# show tibble
annual_summary

# And here is the same result printed using kable
kable(annual_summary, 
      # only show 2 digits
      digits = 2)
```

Note: Consider estimating these values in multiple ways is to help you make sure you are reporting the correct values and that you completely understand your output. When multiple approaches to the same summary give the same answer, you can be more confident that your answer is correct.

### Plots: Histograms

#### Make a histogram of concentrations. Add a density curve to the histogram.

`ggplot2` (part of `tidyverse`) has great tools for plotting. First we show a basic histogram. Then we switch to the density scale and overlay a normal density with the same mean and variance as the data. Finally we overlay a density plot.

See chunk comments and the notes following the chunk for option suggestions.

`ggplot` gives warnings about omitted data and messages about better options to choose. You can prevent these messages from showing up in your rendered document by using the chunk option `warning=FALSE` and `message=FALSE`. They will show up in the Markdown console instead if you set these to `FALSE`. (Note: We set these options globally in our setup chunk above so we don't need to repeat them below.)

```{r hist.in.tidyverse, warning=FALSE, message=FALSE}
#-----hist in tidyverse---------

# Plot 1 with histogram only and count on the y axis (the default) the default
# binwidth is 30 and often you will want to change it 
annual %>%
  filter(variable== "no2") %>%
  ggplot(data=., aes(x=value, na.rm=TRUE), ) +
  geom_histogram(position = "dodge", fill = "purple")

# Plot 2 of density base plot 
# named 'p' for ease of re-use and a different theme:
p <- annual %>%
  filter(variable == "no2") %>%
  ggplot(aes(x = value, na.rm = TRUE)) +
  # Add the density curve with some transparency
  geom_density(alpha = 0.5, color = "purple") +  
  theme_classic()

p


# Calculate the mean and standard deviation of the 'value' column
mean_value <- mean(annual$value[annual$variable == "no2"], na.rm = TRUE)
sd_value <- sd(annual$value[annual$variable == "no2"], na.rm = TRUE)

# Create the density plot with the normal distribution overlay
# The purple line is a kernel density estimate representing the true shape of your data, while the red dashed line assumes a perfect normal distribution with the same mean and standard deviation.
p <- annual %>%
  filter(variable == "no2") %>%
  ggplot(aes(x = value, na.rm = TRUE)) +
  # Add the density curve with some transparency
  geom_density(alpha = 0.5, color = "purple") +
  # Add the normal distribution curve using stat_function
  stat_function(fun = dnorm, args = list(mean = mean_value, sd = sd_value), 
                color = "red", size = 1, linetype = "dashed") +
  geom_vline(xintercept = mean_value, linetype = "dotted") +
  theme_classic()

# Display the plot
p  

 

#  a few style options, such as axis expansion so the bars aren't "floating" and an overall theme.
p +
  scale_y_continuous(expand = expand_scale(mult = c(0, .1))) 


```

Note: In contrast to the normal density which overlays a probability distribution of a specific form, kernel density smoothers give you smooth curve that track the data. The smooth curve will be rougher for smaller bandwidths.

To set the amount of smoothing (i.e. the bandwidth) in the kernel density, use the `bw` option. Kernels are scaled so that `bw` is the standard deviation of the smoothing kernel. To choose the smoothing kernel, use the `kernel` option, e.g. `kernel="gaussian"` (which is the default).

## Create transformed data

Typically exposure data appear to be log-normally distributed. In this section we transform and plot the transformed data.

### Logarithmic transformations using `dplyr`:

-   `mutate(my_dataset, ln_variable = log(variable) )` creates the natural log-transformed variable in the DEMS data frame.
-   `mutate(my_dataset, log10_variable = log10(variable) )` creates the base 10 log-transformed variable in the DEMS data frame.

Note that `mutate` adds the new variables to the end of the dataset. For more functions you can use with `mutate` to create new variables, see R4DS pp. 56-58.

```{r transform.vars}
#-----transform vars------

# The following two variables will be added to the dataset at the end. We are not saving these here, just showing the output
annual %>% 
  mutate(ln_value = log(value), 
         log10_value = log10(value) )

```

## More plots

### Overview of `ggplot` basics

Here is the basic format of `ggplot`:

```         
ggplot(data = <DATA> ) +
    <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))
```

where `<DATA>` is your dataset, `aes` is the aesthetic for the mapping or the visual properties of the objects in the plot, such as size, shape, and color of points. The `mapping` is how you apply the variables in the dataset to the aesthetics. Finally, the `<GEOM_FUNCTION>` is how the mapping will be presented. Examples are `geom_point` and `geom_line`. `ggplot` is very expandable, allowing you to layer aspects into your plots, such as to overlay density plots on top of histograms as we did above. More generally, the layered grammar of `ggplot` is:

```         
ggplot(data = <DATA> ) +
    <GEOM_FUNCTION>(
        mapping = aes(<MAPPINGS>),
        stat = <STAT>,
        position = <POSITION>
    ) +
    <COORDINATE_FUNCTION> +
    <FACET_FUNCTION> +
    <LABEL_FUNCTION>
```

where the added pieces are `<STAT>`: statistics to put on the plot, `<POSITION>` to locate your object (such as `"jitter"`"), `<COORDINATE_FUNCTION>` to specify the coordinate system to plot under (e.g. polar coordinates), `<FACET_FUNCTION>` to allow you to divide the plot into subplots, and `<LABEL_FUNCTION>` in order to add titles, axis labels, etc.

For more details on `ggplot` see R4DS, the [R for Data Science](https://r4ds.had.co.nz/data-visualisation.html) book by Hadley Wickham and Garrett Grolemund.

### Scatterplots: Plot the relationship between NO2 and PM2.5

`ggplot2` makes this easy:

```{r scatterplot.in.tidyverse, echo=TRUE, warning=FALSE, message=FALSE}
#-----scatterplot in tidyverse------------

# make each pollutant of interest its own column, for easier plotting
annual_wide <- annual %>%
  filter(variable %in% c("no2", "pm2.5_ug_m3")) %>%
  pivot_wider(names_from = variable, values_from = value)


# Plot 1: simple way to use ggplot to ask for a scatterplot
qplot(no2, pm2.5_ug_m3, data=annual_wide) 

# Plot 2 is the same plot, now using standard ggplot notation
ggplot(data = annual_wide, aes(no2, pm2.5_ug_m3, na.rm = TRUE)) + 
  geom_point()

# Plot 3 adds a best fit linear fit added without its 95% CI (se=FALSE option). We
# also add a title and axis labels
ggplot(data = annual_wide, aes(no2, pm2.5_ug_m3, na.rm = TRUE)) + 
  geom_point() + 
  stat_smooth(method = lm, se = FALSE) +
    labs(title = "Scatterplot of the NO2 (ppb) vs. PM2.5 (ug/m3) data",  
         x = "NO2 (ppm)",
         y = "PM2.5 (ug/m3)")

# Plot 4 adds a smooth loess curve added and its 95%
# CI (the default smoother is gam for large datasets and loess when there are
# less than 1,000 observations)
ggplot(data = annual_wide, aes(no2, pm2.5_ug_m3, na.rm = TRUE)) + 
  geom_point() + 
  stat_smooth(method = lm, se = FALSE) +
  stat_smooth(method = "loess", col="green") +
    labs(title = "Scatterplot of the NO2 (ppb) vs. PM2.5 (ug/m3) data",  
         x = "NO2 (ppm)",
         y = "PM2.5 (ug/m3)")

 
# Plot 6 adds facets + a new color theme for site type
# Note 1:  if you wanted, the scales="free" option in facet_wrap() allows a different scale for each plot, which sometimes
# can help plots be more informative. 
annual_wide %>%
  # create a new variable to indicate if the location was a stop site or specifically a collocation site for quality control purpuses
  mutate(site_type = ifelse(grepl("MS", location), "stop site",
                       ifelse(grepl("MC", location), "collocation site", NA))) %>%
  
  ggplot(data = ., aes(no2, pm2.5_ug_m3, col=site_type)) + 
  geom_point() + 
  facet_wrap(~site_type) +
  stat_smooth(method = "loess", se = FALSE) +
    labs(title = "Scatterplot of the NO2 (ppb) vs. PM2.5 (ug/m3) data",  
         x = "NO (ppm)",
         y = "PM2.5 (ug/m3)")
    
```

------------------------------------------------------------------------

# Homework exercises

Note: Refer to the lab write-up guidelines *LabReportGuidelines.html* posted on Canvas for the format and content of your lab report.

(1) Make table(s) summarizing each pollutant ('variable') in this dataset. Include the arithmetic mean (AM), arithmetic standard deviation (ASD), geometric mean (GM), and geometric standard deviation (GSD) of each pollutant. Write a few sentences describing the results in the table. 

    (a) Note: This is not a concern in this dataset, but 0 values are common in many environmental datasets. You can't take the log of 0 so you would need to decide how to handle the 0's. Summary statistics from the reduced dataset, i.e. a dataset created by omitting the 0's, will be misleading. A simple alternative is to create a new variable that adds a constant to every observation and use this variable when taking logs. However, the purpose of your analysis matters since adding a constant is not always the appropriate way to handle this challenge. If you do add a constant, choose the constant to add thoughtfully. How big should it be? Should it be the same for all pollutants? Make sure to document any changes to the variables in your lab write-up.

    (b) Getting GM and/or GSD in a table in R: This lab doesn't show how to get the GM and GSD using `tidyverse`; for example using `dplyr` `group_by()` and `mutate()` functions though you could (and may wish to). Instead, for demonstration, we show how to use the `EnvStats` package with `tapply` and `cbind`. To get the AM, ASD, GM, GSD, N in order:

Repeat this exercise for normal stop versus collocation sites (see example code above for creating this variable).  


(2) Make some figures to show the distribution of the air pollution measurements ('value'). (Challenge version: Can you do this separately by site_type?) Write a few sentences describing what you see in the figures.


------------------------------------------------------------------------

# Appendix: Session Information, Code, and Functions {.unnumbered}

The next three chunks should be included in the appendix of every Markdown so that you document your session information, code, and functions defined in the document. This supports the reproducibility of your work.

```{r session.info}
#-----session information: beginning of Appendix -----------

# This promotes reproducibility by documenting the version of R and every package
# you used.
sessionInfo()

```

```{r appendix, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, include=TRUE}
#-----appendix------------
```

```{r functions.defined.in.this.qmd, eval = TRUE}
#-----functions defined in this Rmd ------------

# Show the names of all functions defined in the .qmd
# (e.g. loaded in the environment)
lsf.str()

# Show the definitions of all functions loaded into the current environment  
lsf.str() %>% set_names() %>% purrr::map(get, .GlobalEnv)

```
