---
title: "Week 2 Lab:  Compliance -- Descriptive Statistics and exceedance Probabilities"
author: "Instructors for ENVH 556 Autumn 2024"
format:
  html:
    df_print: "paged"
    fig_caption: yes
    toc: true
    toc_depth: 3
    number_sections: true
    self-contained: true #save images etc. in this file 
execute:
  echo: true
  cache: false
  echo.comments: false
  message: false
  warning: false
  output: console  # Show the output in the console instead of inline

---

This document was rendered on `r format(Sys.time(), '%B %d, %Y')`.


```{r clear.workspace, echo=FALSE}
#-----clear workspace----

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
    res <- suppressWarnings(lapply(
        paste('package:', names(sessionInfo()$otherPkgs), sep = ""),
        detach,
        character.only = TRUE,
        unload = TRUE,
        force = TRUE
    ))
}

```

```{r load.packages, include=FALSE}
#-----load packages----

# To address a dependency issue, one package (`limma`) must be installed through
# another package (`BiocManager`)
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("limma")

# For the remaining packages, load pacman, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.  Some reasons for packages:
# knitr:  kable
# kableExtra: kable_styling()
# dplyr: data wrangling functions.
# tidyr: drop_na
# ggplot: visualizations
# Hmisc:  describe
# EnvStats: geoMean, geoSD, probability plotting functions
# MKmisc:  quantileCI
pacman::p_load(dplyr, tidyr, knitr, kableExtra, purrr, ggplot2, Hmisc, EnvStats, MKmisc)

# download_and_read_file() function etc.
source("global_functions.R")

```

```{r read.data, echo=FALSE}
#-----read data----

# create data directory if it does not exist
dir.create(file.path("Datasets"),
           showWarnings = FALSE,
           recursive = TRUE)
           
# read data; download if necessary
DEMS <- download_and_read_file(data_url = "https://faculty.washington.edu/sheppard/envh556/Datasets/DEMSCombinedPersonal.rds", 
                                 output_file_path = file.path("Datasets", "DEMSCombinedPersonal.rds"))
           
```

# Purpose  

The purpose of this lab is to work with descriptive statistics and compliance tests while also getting further practice with R, RStudio, and Markdown.  We will use the DEMS REC data, describing the distributions, exceedance probabilities, confidence intervals, and test compliance.

# The DEMS Data       
The underlying data to be used in the lab is from the DEMS study and is described in [Coble et al 2010](https://academic.oup.com/annweh/article/54/7/747/202635).  We are focusing on the personal data collected by NIOSH between 1998 and 2001.  In this lab we will mostly use the `ecdata` variable which is a measurement of exposure to respirable elemental carbon, also called REC.  The data were collected from workers from a cross-section of jobs at the seven mines open during the data collection.  For further information on this dataset, see the document: *DEMS Personal Data overview* available on the class Canvas site.


**Variable names and explanations:**      
- Facility:     
  - facilityno: Factor variable with levels H6, G7, I8, E13, A15, B16, J18, D21    
  - facility ID: Character variable with values A, B, D, E, G, H, I     
- ecdata: REC (Respirable Elemental Carbon), in Âµg/m3 (double precision)     
- u_s: Character variable indicating underground (u), surface (s), or mixed (m)    
- job: Integer indicating the job number; job titles were discerned, but crosswalk was not provided    

Let's briefly review the data: 

```{r data.description.1}
#-----data description 1---------- 
class(DEMS) 

# We expect this to have 1275 rows and 17 columns
dim(DEMS) 

# This list should correspond to the list in the data summary document:
names(DEMS) 

# # first 6 rows and ALL variables (columns):
head(DEMS) 

# a basic summary of each variable
## Question: compare the ecdata mean and median. What does this tell you about the potential distribution of ecdata in terms of normality? 
summary(DEMS) 

# a different basic summary of each variable from `Hmisc`
describe(DEMS) 

```

# Getting Started  
This section gives formulas and some basic R commands for descriptive statistics and exceedance probabilities.  It also provides reminders of formulas from lecture you will use in lab.

## Definitions

* AM = arithmetic mean 
  * MOM = method of moments (lognormal samples)
  * MLE = maximum likelihood estimate (lognormal samples)
* GM = geometric mean  
* SD = standard deviation (on the native scale)
* GSD = geometric standard deviation
* OEL = occupational exposure limit
* CI = confidence interval

## Formulas for method of moments vs MLE estimates for the AM  

Methods for estimating AM from a lognormal sample (uses the mean and variance on log-transformed data).

* AM method of moments (MOM) estimate for `ecdata` = $x$: 

$$\bar{x}_{MOM} = \frac{1}{n} \sum_{i=1}^{n} x_i $$

* AM maximum likelihood estimate (MLE) for `ecdata` = $x$ using
`lnrec` = `log(ecdata)` = $y$:   

$$ AM_{MLE}=\exp\big(\mu_y+\frac{\sigma_y^2}{2}\big)$$

$$ \bar{x}_{MLE}=\exp{\big(\bar{y}+\frac{\frac{N-1}{N}s_y^2)}{2}\big)}$$

## Formulas for the exceedance fraction   
* **Empiric exceedance fraction**:  For a sample of size *N* where *n* is the number exceeding the OEL, calculate

$$f_{OEL}=\frac{n>\mbox{OEL}}{N}$$

* **Parametric exceedance fraction**:  For a sample, log-transform the data and OEL and use the normal distribution to estimate the probability  of exceeding ln(OEL):

$$P\big(y>\ln(OEL)\big) =
\big(1-\Phi(z=\frac{y-\bar{y}}{s_y}>\frac{\ln(OEL)-\bar{y}}{s_y})\big)$$

## Basic data manipulation and summarization commands   
(See also Week 1 lab)

(@) **Keep a subset of observations:**  Keep only a subset of the data based on selected jobs (240,110,410,600), underground only ("u"), and `ecdata` being non-missing:

```{r filter.data}
#-----filter data----

# filter dataset to keep underground only; 4 jobs codes: 240,110,410,600; and 
# non-missing ecdata. 
DEMSu <- filter(DEMS, 
                u_s == "u", 
                job %in% c(240, 110, 410, 600), 
                !is.na(ecdata))

```

(@) **Create new variables** using the log transformation.  Typically exposure data appear to be log-normally distributed.

```{r transform.vars}
#-----transform vars----

# The natural log and log base 10 transformations of ecdata will be added to the 
# DEMSu tibble:
DEMSu <- mutate(DEMSu,
                lnrec = log(ecdata),
                log10rec = log10(ecdata) )

# check the new data columns
DEMSu %>%
  select(ecdata, lnrec, log10rec) %>%
  head()

```

(@) **Summarize variables and display key quantities:** This code uses `dplyr`,
part of `tidyverse`.  We use `group_by()` to determine the subgroups we are going to summarize over, `summarise()` to create new summaries we want to report, `mutate_if()` to ease rounding all variables of class `double`, and `arrange()` to decide the final ordering in the table.  Each of these is connected though the pipe operator (`%>%`) which can be read as "then".  It facilitates the process of connecting multiple steps without creating intermediate datasets by using the output of one function as the input of the next function in the "pipeline."

```{r table.with.dplyr}
#-----table with dplyr----

# create summary 
# note: "summarize()" will call a different function from the Hmisc package
DEMSsummary <- DEMSu %>% 
    group_by(job,facilityid) %>%
    summarise(N = n(),
              AM = mean(ecdata),
              # adjusted MLE approach for estimating AM using the log mean and log SD
              AM_mle = (exp(mean(lnrec)+0.5*((N-1)/N)*(sd(lnrec)^2))),
              GM = geoMean(ecdata),
              Median = median(ecdata),
              ASD = sd(ecdata),
              GSD = geoSD(ecdata),
              .groups = "drop") %>% 
    mutate_if(is.double, round, digits = 2) %>% 
    arrange(job, facilityid, desc(AM))

# print summary using kable
## compare the different estimates of central tendency 
## question: why do samples with N=1 have AM_mle missing? - you can't take the SD of only one value
kable(DEMSsummary,
      caption = "Distribution of EC by job and facility",
      digits=1) %>%
  kable_styling()
   

```

## Visualize the data and understand its distribution graphically

Here are some sample graphical commands: (Plots in this section are not run; histogram code is modified from Week 1 lab, though now with new data and better labeling.)

### Histograms

```{r hist.in.tidyverse, echo=TRUE,eval=FALSE}
#-----hist in tidyverse----
# plots on the native scale - data are right skewed
ggplot(data = DEMSu, aes(ecdata)) +
    geom_histogram(colour = "black", 
                   fill = "white") 


# log REC
p <- ggplot(data = DEMSu, aes(lnrec)) +
    geom_histogram(aes(y = ..density..), 
                   colour = "black", 
                   fill = "white", 
                   binwidth = 0.5 
                   )
p

# create dataframe to overlay a normal density curve:
norm_df <- with(DEMSu %>% drop_na(lnrec), 
           tibble(x = seq(min(lnrec), max(lnrec), length.out = length(lnrec)), 
                  # calculate the density of a normal distribution for a given set of values
                  y = dnorm(x, mean(lnrec), sd(lnrec)) ) 
           )

# histogram + density + overlaid normal - these look similar
p +
  geom_density(alpha = .2, linetype=2) +
  geom_line(data = norm_df, aes(x = x, y = y), color = "red")  +
  labs(title = paste("Density Plot of ln(REC)\n","In 4 Underground Jobs"),
       caption = paste("With overlaid normal distribution (red line) with the same mean and SD"),
       x = "ln(REC) (ln(ug/m^3)") +
  scale_y_continuous(expand = expansion(mult = c(0, .1))) +
  theme_classic() +
  # left justify the caption
  theme(plot.caption = element_text(hjust = 0))
    
```

### Q-Q plots (Normal probability plots)

The y-axis shows the sorted values of your data, plotted against the theoretical quantiles on the x-axis.


```{r qqplot.in.tidyverse, echo=TRUE,eval=FALSE}
#-----qqplot in tidyverse----

# create the base plot, overlay the points and line, and add title
## using log REC
ggplot(DEMSu, aes(sample = lnrec)) +
  stat_qq() + 
  stat_qq_line() +
  labs(title = "Normal Q-Q Plot of ln(REC)\nIn 4 Underground Jobs",
       x = "Theoretical Quantiles from a Std Normal Distribution",
       y = "Log REC Conc"
       )

```

What does the deviation of the lower tail from the reference line indicates?

```{r}
# Answer: the distribution of the log-transformed concentrations (ln(REC)) has more low values than would be expected if the data followed a normal distribution (see previous plot).

```

##  Test the normality of a variable 

The Shapiro-Wilk test evaluates the null hypothesis that the data are normally distributed against the alternative that they aren't.  Unfortunately, **it is very easy to reject the null hypothesis using this test**, particularly when the sample size is large.  See [this example using simulations](http://emilkirkegaard.dk/en/?p=4452) to get a feeling for the kinds of deviations from normality that give high and statistically significant Shapiro-Wilk tests.  **The general advice in using this test is to not over-interpret it.**  Look at the data visually and use your judgment about whether they are consistent with a normal distribution.  Ask yourself whether it is just a few values at the tails that deviate, or are there other more important concerns.


**Interpretation**: If the p-value is less than or equal to the significance level (e.g., p < 0.05), you reject the null hypothesis, suggesting that the data significantly deviates from a normal distribution. This could indicate that the data is skewed, has heavy tails, or contains outliers. 

```{r Shapiro-Wilk test}
#-----Shapiro-Wilk test----

#for normality of a variable
shapiro.test(DEMSu$lnrec)

```

## Take a random sample of data 

* `small<-sample(DEMSu$lnrec,10)` gives a sample of size 10 of the data
* `half<- sample(DEMSu$lnrec,size=length(DEMSu$lnrec)/2,replace=TRUE)` gives a
50% sample of the data with replacement

```{r samples}
#-----samples----

# recall you want to set the seed to make your work reproducible
## note that different versions of R could theoretically produce different results
set.seed(502)

# the sample command takes from the vector of data (the first argument), a
# sample of size given by the argument.  The default is to sample without
# replacement, so you need to set replace=TRUE if you want sampling with
# replacement.
small <- sample(DEMSu$lnrec, size = 10)
half <- sample(DEMSu$lnrec,
               size = length(DEMSu$lnrec) / 2,
               replace = TRUE)

```


## Calculate the exceedance fraction and related statistics

Here we get the empiric exceedance fraction, the parametric exceedance probability and show the tools for estimating various confidence limits for percentiles. For these calculations, **we'll assume an OEL for REC equal to 200 $\mu g/m^3$**.

* **Empiric exceedance fraction**: 

Use the *observed data* to estimate the probability of exceeding a certain threshold (log(200) in this case).

This approach makes no distributional assumptions about the data. 

```{r emp.exc.frac}
#-----emp exc frac----
# proportion of samples above a specific value
sum(DEMSu$ecdata > 200) / length(DEMSu$ecdata)

```

* **Parametric exceedance fraction**: 

Fit a statistical distribution (e.g., normal, log-normal) to the data and use the fitted distribution to estimate the probability of exceeding the threshold.

**aside**: example of a normal distribution, z-scores, and quantiles

```{r}
# Create a sequence of z-scores from -3.5 to 3.5
z_values <- seq(-3.5, 3.5, length.out = 1000)

# Calculate the density of the normal distribution for these z-scores
density_values <- dnorm(z_values)

# Create a data frame for plotting
data <- data.frame(z = z_values, density = density_values)

# Plot the normal distribution
p <- ggplot(data, aes(x = z, y = density)) +
  geom_line(color = "blue") +
  labs(
    title = "Normal Distribution with Z-scores",
    x = "Z-score",
    y = "Density"
  ) +
  theme_minimal()

p

# Add shaded regions for specific quantiles
p <- p + stat_function(
  fun = dnorm,
  geom = "area",
  fill = "lightblue",
  alpha = 0.5,
  xlim = c(-1, 1)  # Example for showing the area within 1 SD
)


# Add annotations to show translation of z-scores to quantiles
p <- p + geom_vline(xintercept = c(-1, 0, 1), linetype = "dashed") +
  annotate("text", x = -1, y = 0.1, label = "Z = -1\n(16th percentile)", color = "red", size = 3, hjust = 1.2) +
  annotate("text", x = 0, y = 0.2, label = "Z = 0\n(50th percentile)", color = "red", size = 3, hjust = 0.5) +
  annotate("text", x = 1, y = 0.1, label = "Z = 1\n(84th percentile)", color = "red", size = 3, hjust = -0.2)

p

```


```{r param.exc.frac}
#-----param exc frac----

# 1. Convert log(200) into a z-score, to indicate how many standard deviations it is away from the observed log mean.  Standardize by subtracting the mean of lnrec and then dividing by its standard deviation.
(z_score <- (log(200) - mean(DEMSu$lnrec)) / sd(DEMSu$lnrec))
# 2. the proportion of the data (or the area under the curve) that lies to the left of the given z-score
(pnorm_result <- pnorm(z_score))

# 3. the probability that this is greater than a z-score.
1 - pnorm_result

```

* **95^th^ percentile of the distribution and its 70% CI**.  The following describes an approach estimated directly on the log-transformed data and exponentiates.  You can use the `quantileCI` command in the `MKmisc` package. (See https://rdrr.io.)  The `quantile` command in base R can be used to get the 95th percentile (upper 5th percentile), but there is no clear way to get its CI from the `quantile` command.

```{r quantile.95+70th.CI}
#-----quantile 95+70th CI----

# Using the quantile command:
# 95th percentile quantile on log scale
quantile(DEMSu$lnrec, .95)

# 95th percentile quantile on native scale
exp(quantile(DEMSu$lnrec, .95))

# now using quantileCI:
## function uses bootstrap sampling (sampling with replacement) to calculate an interval. For each resampled dataset, the specified quantile is calculated.The resulting distribution of quantiles is used to determine the confidence interval. 
### log scale
quantileCI(DEMSu$lnrec, prob = 0.95, conf.level = 0.7)
### native scale
quantileCI(DEMSu$ecdata, prob = 0.95, conf.level = 0.7)

```

What is the interpretation of these results?

```{r}
# the point estimate for the 95th percentile is 400 Âµg/mÂ³, but there is some uncertainty. The interval 349-406 Âµg/mÂ³ captures where the true 95th percentile is likely to fall with 70% confidence.
```

How would this range change if the we wanted to have more confidence (have more certainty) of the true 95th percentile?

```{r}
# The CI would be come wider
```


## Log probability plots

These plots have the value of the exposure variable on the native scale (e.g. concentration, not transformed) on the x axis displayed on the log base 10 scale, and the corresponding normal probability for the cumulative distribution on the y axis.  The following code generates some lognormal data and then plots them using this framework.  To implement this with the DEMS data, you will need to address specifics in the example, such as locations of the tick lines and range of the data.

Notes on how to create this: 

1. Focus on x, our exposure variable of interest, which is typically assumed to be lognormally distributed  

2. Transform data to the log scale ($y=ln(x)$) 

3. Generate order statistics ($p_i=order/N+1$)

4. Convert these order statistics to standard normal quantiles with the same mean and SD as y.

5. Exponentiate the normal quantile variable so it is comparable with x.

6. Plot the theoretical quantiles (exponentiated quantiles) vs the input data x (data on the x axis; theoretical quantiles on the y axis).

7. Plot labels on y axis shows the corresponding normal probabilities rather than the theoretical quantiles.  Both axes scale to the log base 10 scale. 

```{r generate data for log probability plot}
#-----generate data for log probability plot example----

# set seed for reproducibility
set.seed(2001)

# the measurement x ~LN(mu_y,sd_y) and y=log(x) ~N(mu_y,sd_y)

# x is the "measured" exposure data ~LN
# generate random numbers from a log-normal distribution. 
x <- rlnorm(n = 1000, meanlog = 1, sdlog = 0.92)

############## side note ##############
# note that the numbers generated are on the native (non-log scale). The mean is ~4, and max ~63
summary(x)
# the numbers produced are right skewed
hist(x)
# ...but normal on the log scale
hist(log(x))
######################################

# variable y is normally distributed 
# for exposure data this is ordinarily the log-transformed measurement
y <- log(x)

# Use rank to get the order statistics 
# the smallest (min) number is 1, the largest (max) number is length(x)
rx <- rank(x)

# order statistics re-expressed as proportions 
p_i <- rx/(length(x)+1)

# small values are small proportions, large values produce large proportions
range(p_i)


# log mean & SD
(y_bar <- mean(y))
(sd_y <- sd(y))

# theoretical quantiles of the normal distribution that corresponds to the data on the log scale
# qnorm() returns z-scores (quantiles) for probabilities
z_scores2 <- qnorm(p_i)

summary(z_scores2)

# convert to log concentrations
qy <- z_scores2*sd_y+y_bar
# summary(qy)

# native scale concentrations
qx <- exp(qy)
 
# create a data frame for plotting
pplot_data <- tibble(y, x, rx, p_i, qy, qx)

# some summary statistics to check while developing this (commented out)
#summary(y)
paste("GM:  ", exp(y_bar))
paste("GSD:  ",exp(sd(y)))
paste("AM:  ", exp(y_bar+((length(y)-1)/length(y))*sd_y^2/2))
#summary(x)
#sd(x)

# now generate the data for the y axis -- need to create a vector of probabilities
# that we wish to display:
probs <- c(.01, .02, .05, .1, .16, .25, .5, .75, .84, .9, .95, .98, .99)

# get the corresponding quantiles for the normal distribution with the same mean and variance
quants <- qnorm(probs, mean = y_bar, sd = sd_y)

# exponentiate these for plotting
exp_quants <- exp(quants)

# in the plots we will draw horizontal lines at exp_quants and label these lines with the probs

```

This log probability plot assess whether the data follow a specific distribution, in this case, the log-normal distribution. **It should since we simulated these data to be log normal.**

**Consistent deviations in the tails can suggest that the distribution used (log-normal in this case) might not characterize the data best.** 

The **black line** represents the theoretical cumulative distribution function (CDF) of a log-normal distribution fitted to the data. Ideally, if the data perfectly follow the log-normal distribution, the points should lie close to this line.

**Outliers:** Any significant deviations from the line, especially towards the upper end of the plot, may indicate the presence of outliers or that the data has a heavy tail.

Points above the reference line suggest the observed values are larger than would be expected for a normal distribution. If they lie below the line, the observed values are smaller than expected. 

```{r log probability plot}
#-----log probability plot----

# Shows percentiles of the cumulative normal on the y axis; 
# coord_fixed assumes both axes are the same scaling (i.e. the aspect ratio for x, qx is the same);
# annotation_logticks puts in minor ticks in right scaling
# minor_breaks addresses the unlabeled grid lines
p <- ggplot(data = pplot_data) +
    geom_point(aes(x,qx), shape = "o", alpha = 0.4) +
    geom_line(aes(qx,qx)) +
    annotation_logticks(sides="b") +
    # axes on the log base 10 scale;
    scale_x_log10(breaks=c(0.1,0.5,1,5,10,50),
                  limits=c(0.1,50),
                  minor_breaks=c(0.2,1,2,20) ) +
    scale_y_log10(breaks=exp_quants,
                  labels=probs,
                  limits=c(0.1,50),
                  minor_breaks=NULL) +
    coord_fixed() +
    labs(title="Sample log probability plot\nUsing simulated data",
         x = "Concentration (native scale units)",
         y= "% of data less than") +
    theme_bw()
p

```


# Practice Session  

1) Using the âDEMSCombinedPersonalâ R dataset, keep only measurements from jobs 110, 240, 410 and 600 among underground workers.  (We have selected these particular jobs for simplicity, but feel free to explore additional jobs or other categories of the data if you want to.)  In order to avoid potential confusion later, for this practice session we suggest you also drop any observations that are missing `ecdata`.
2) Describe REC (varname: `ecdata`)
3) Determine whether REC in this subset is lognormally distributed. Explore using histograms, qqplots,  and statistical tests.
4) Calculate the GM, GSD, and AM (using both method of moments and maximum likelihood estimates for the AM) for each group.
5) For a selected group (i.e. a single job, or for all four if you want): Assume data are lognormal (LN) and an OEL of 200 $\mu g/m^3$ has been determined for REC.
    a) Calculate the empiric and parametric exceedance probabilities along with
    the 95^th^ percentile and 70% confidence limits.
    
    b) Take a random sample of 50%, 25%, n=9 and n=5 samples and recalculate the
    GM, GSD, 95^th^ percentile $\pm$ 70% confidence limits.

# Homework Exercises

1) Consider the primary exposure measures of interest to the study, including
REC, NO_2 and organic carbon.  Choose either the full dataset or a reduced subset, such as the four selected underground jobs.  (Justify your choice as part of your lab write-up.)   
    a) Describe the distribution, out of range and/or missing values for these exposure measures. 
    b) Determine the adequacy of the LN distribution for representing these using  
distribution plots and/or statistical tests.   
2) Explore potential stratification variables (determinants such as facility, job, location). 
    a) Do the stratified data improve the distributional characteristics?  Does it matter whether you  restrict your attention to smaller subgroups of the data, e.g., underground only, specific facilities, specific jobs, or a combination of these?  
    
3) Calculate the GM, median, AM (method of moments) and AM (maximum likelihood)
for REC.    
    a) How do these quantities compare to each other?   
    b) Can you determine any characteristics of the data which help explain the differences between these alternative measures of central tendency?  
    c) For at least one stratum that has data that aren't too far off from a lognormal distribution, plot the log probability plot.  Read off the GM and GSD from the log probability plot and compare these to values you estimate directly from the data.  
4) Assuming an OEL for REC of 200 $\mu g/m^3$, calculate the exceedance probability for each mine (and/or job) using both empiric and parametric
approaches.  In addition, calculate the 95^th^ percentile of the distribution, and
provide 70% confidence limits on these percentiles.
    a) What are the differences between the empiric and parametric methods of calculation?  
5) Take random samples of the data (e.g., 50%, 10%, n=9, n=5) and recalculate
the various summary statistics. 
    a) How does the reduced sample size affect the estimates?  Explain.   



# Appendix 1:  Older Base R versions for reference {-}

## A1.1 Q-Q plot {-}

For a q-q plot to determine whether the data are normally distributed, use `qqnorm()` and you can overlay `qqline()`.  There is code in the `.Rmd` file if you wish to use it.

```{r qqplot.in.baseR, echo=FALSE, eval=FALSE}
#-----qqplot in baseR----

# apparently qqnorm works fine in the presence of missing data:
qqnorm(DEMS$ecdata)
qqline(DEMS$ecdata, col = "red", lwd = 2)

# now repeat using the DEMSu subset of data since they should be closer to
# normal
qqnorm(DEMSu$ecdata)
qqline(DEMSu$ecdata, col = "red", lwd = 2)

```


# Appendix 2:  Code, session information, and functions {-}

```{r session.info}
#-----session information----

# print R session information
sessionInfo()

```

```{r code.appendix, ref.label = knitr::all_labels(), echo = TRUE, eval = FALSE, , include=TRUE}
#-----code appendix----
```

```{r functions, eval = TRUE}
#-----functions----

# Show the names of all functions defined in the .Rmd
# (e.g. loaded in the environment)
lsf.str()

# Show the definitions of all functions loaded into the current environment  
lsf.str() %>% set_names() %>% map(get, .GlobalEnv)

```
